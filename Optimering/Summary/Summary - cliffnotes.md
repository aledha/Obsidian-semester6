[PDF](https://wiki.math.ntnu.no/_media/tma4180/2023v/summary.pdf)
## Theory of optimization:
- [[Basics]]: The study of finding the best solution to a problem.
- [[Unconstrained problems]]: Finding the best solution without any restrictions.
- [[Constrained optimization]]: Finding the best solution subject to certain constraints.
- [[Convex optimization]]: Finding the best solution to a problem with a convex objective function and convex constraints.
- [[Lagrangian duality]]: A technique for solving constrained optimization problems using Lagrange multipliers.
- [[Multi-criteria optimization]]: Finding the best solution to a problem with multiple objectives to be optimized simultaneously.


## Numerical optimization:
- [[Line search methods]]: Iteratively adjusting the step size towards the optimum along a given search direction.
- [[CG and quasi-Newton]]: Optimization methods that utilize gradient and Hessian information to iteratively improve the solution.
- [[Trust region methods]]: A method for optimizing with constraints by constraining the search region.
- [[Non-linear least squares methods]]: A method for finding the best fit of a function to data that involves minimizing the sum of the squares of the residuals.
- [[Penalty and barrier methods]]: Methods for solving constrained optimization problems by transforming them into unconstrained problems with penalized or barrier functions.
- [[Linear and quadratic programming]]: Solving optimization problems with linear or quadratic objective functions subject to linear constraints.
- [[Sequential quadratic programming]][[:]] An iterative method for solving nonlinear constrained optimization problems by approximating the objective function with a quadratic function in the neighborhood of each point.